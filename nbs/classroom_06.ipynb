{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classroom 6 - Training a Named Entity Recognition Model with a LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classroom today is primarily geared towards preparing you for Assignment 4 which you'll be working on after today. The notebook is split into three main parts to get you thinking. You should work through these sections in groups together in class. \n",
    "\n",
    "If you have any questions or things you don't understand, make a note of them so you can remember to ask - or, even better, post them to Slack!\n",
    "\n",
    "If you get through everything here, make a start on the assignment. If you don't, dont' worry about it - but I suggest you finish all of the exercises here before starting the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A very short intro to NER\n",
    "Named entity recognition (NER) also known as named entity extraction, and entity identification is the task of tagging an entity is the task of extracting which seeks to extract named entities from unstructured text into predefined categories such as names, medical codes, quantities or similar.\n",
    "\n",
    "The most common variant is the [CoNLL-20003](https://www.clips.uantwerpen.be/conll2003/ner/) format which uses the categories, person (PER), organization (ORG) location (LOC) and miscellaneous (MISC), which for example denote cases such nationalies. For example:\n",
    "\n",
    "*Hello my name is $Ross_{PER}$ I live in $Aarhus_{LOC}$ and work at $AU_{ORG}$.*\n",
    "\n",
    "For example, let's see how this works with ```spaCy```. NB: you might need to remember to install a ```spaCy``` model:\n",
    "\n",
    "```python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello my name is Ross. I live in Denmark and work at Aarhus University, I am Scottish and today is Friday 28th.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hello my name is \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Ross\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ". I live in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Denmark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and work at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Aarhus University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", I am \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Scottish\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    today\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Friday 28th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging standards\n",
    "There exist different tag standards for NER. The most used one is the BIO-format which frames the task as token classification denoting inside, outside and beginning of a token. \n",
    "\n",
    "Words marked with *O* are not a named entity. Words with NER tags which start with *B-\\** indicate the start of a multiword entity (i.e. *B-ORG* for the *Aarhus* in *Aarhus University*), while *I-\\** indicate the continuation of a token (e.g. University).\n",
    "\n",
    "    B = Beginning\n",
    "    I = Inside\n",
    "    O = Outside\n",
    "\n",
    "<details>\n",
    "<summary>Q: What other formats and standards are available? What kinds of entities do they make it possible to tag?</summary>\n",
    "<br>\n",
    "You can see more examples on the spaCy documentation for their [different models(https://spacy.io/models/en)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello O\n",
      "my O\n",
      "name O\n",
      "is O\n",
      "Ross B-PERSON\n",
      ". O\n",
      "I O\n",
      "live O\n",
      "in O\n",
      "Denmark B-GPE\n",
      "and O\n",
      "work O\n",
      "at O\n",
      "Aarhus B-ORG\n",
      "University I-ORG\n",
      ", O\n",
      "I O\n",
      "am O\n",
      "Scottish B-NORP\n",
      "and O\n",
      "today B-DATE\n",
      "is O\n",
      "Friday B-DATE\n",
      "28th I-DATE\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    if t.ent_type:\n",
    "        print(t, f\"{t.ent_iob_}-{t.ent_type_}\")\n",
    "    else:\n",
    "        print(t, t.ent_iob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For the Q:\n",
    "https://towardsdatascience.com/named-entity-recognition-ner-using-spacy-nlp-part-4-28da2ece57c6 this link shows the other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some challenges with NER\n",
    "While NER is currently framed as above this formulating does contain some limitations. \n",
    "\n",
    "For instance the entity Aarhus University really refers to both the location Aarhus, the University within Aarhus, thus nested NER (N-NER) argues that it would be more correct to tag it in a nested fashion as \\[\\[$Aarhus_{LOC}$\\] $University$\\]$_{ORG}$ (Plank, 2020). \n",
    "\n",
    "Other task also include named entity linking. Which is the task of linking an entity to e.g. a wikipedia entry, thus you have to both know that it is indeed an entity and which entity it is (if it is indeed a defined entity).\n",
    "\n",
    "In this assignment, we'll be using Bi-LSTMs to train an NER model on a predifined data set which uses IOB tags of the kind we outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training in batches\n",
    "\n",
    "When you trained your document classifier for the last assignment, you probably noticed that the neural network was quite brittle. Small changes in the hyperparameters could cause massive changes in performance. Likewise, you probably noticed that they tend to substantially overfit the training data and underperform on the validation and test data.\n",
    "\n",
    "One way we can get around this is by processing the data in smaller chunks known as *batches*. \n",
    "\n",
    "<details>\n",
    "<summary>Q: Why might it be a good idea to train on batches, rather than the whole dataset?</summary>\n",
    "<br>\n",
    "These batches are usually small (something like 32 instances at a time) but they have couple of important effects on training:\n",
    "\n",
    "- Batches can be processed in parallel, rather the sequentially. This can result in substantial speed up from computational perspective\n",
    "- Similarly, smaller batch sizes make it easier to fit training data into memory\n",
    "- Lastly,  smaller batch sizes are noisy, meaning that they have a regularizing effect and thus lead to less overfitting.\n",
    "\n",
    "In this assignment, we're going to be using batches of data to train our NER model. To do that, we first have to prepare our batches for training. You can read more about batching in [this blog post](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/).\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One reason might be because of speed. The algorithm has to store too much error values when not splitting it up into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this allows us to look one step up in the directory\n",
    "# for importing custom modules from src\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.util import batch\n",
    "\n",
    "# numpy and pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# loading data and embeddings\n",
    "from datasets import load_dataset\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.LSTM import RNN\n",
    "from src.embedding import gensim_to_torch_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the datset using the ```load_dataset()``` function we've already seen. Here we take only the training data.\n",
    "\n",
    "When you've downloaded the dataset, you're welcome to save a local copy so that we don't need to constantly download it again everytime the code runs.\n",
    "\n",
    "Q: What do the ```train.features``` values refer to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: what do the train.features values refer to?\n",
    "\n",
    "We have the following features in the train df: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags']\n",
    "\n",
    "id refers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ac99faafe864dbcb3a21d62ba66feec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7e5ce846f8450cac25085eda75827d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d016cb81fcd4e84a8b6c231c86213bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conllpp/conllpp (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fb8f091ba948499d52412f7152c6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6144926cad294a8ba321ae894a6181ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/650k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b71698859c14b3994c8c16d02434176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/163k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c066b9a2594a97bbffcaaaeb1fe62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c1747d6987459c8f6e6dfbb0a8379d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2802154a03a54cccb43acc17604cc53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5207a1d45954000b8eb9bfe8e091a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e58db217d34d0b9a10189a67155257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conllpp downloaded and prepared to /home/coder/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb419100b64f45e98ef880dcc8b2518d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DATASET\n",
    "dataset = load_dataset(\"conllpp\")\n",
    "train = dataset[\"train\"]\n",
    "\n",
    "# inspect the dataset\n",
    "train[\"tokens\"][:1]\n",
    "train[\"ner_tags\"][:1]\n",
    "\n",
    "# get number of classes\n",
    "num_classes = train.features[\"ner_tags\"].feature.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'],\n",
       " ['Peter', 'Blackburn'],\n",
       " ['BRUSSELS', '1996-08-22'],\n",
       " ['The',\n",
       "  'European',\n",
       "  'Commission',\n",
       "  'said',\n",
       "  'on',\n",
       "  'Thursday',\n",
       "  'it',\n",
       "  'disagreed',\n",
       "  'with',\n",
       "  'German',\n",
       "  'advice',\n",
       "  'to',\n",
       "  'consumers',\n",
       "  'to',\n",
       "  'shun',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  'until',\n",
       "  'scientists',\n",
       "  'determine',\n",
       "  'whether',\n",
       "  'mad',\n",
       "  'cow',\n",
       "  'disease',\n",
       "  'can',\n",
       "  'be',\n",
       "  'transmitted',\n",
       "  'to',\n",
       "  'sheep',\n",
       "  '.']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the dataset\n",
    "train[\"tokens\"][:4]\n",
    "# train[\"ner_tags\"][:1]\n",
    "\n",
    "# # # get number of classes\n",
    "# num_classes = train.features[\"ner_tags\"].feature.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'pos_tags': Sequence(feature=ClassLabel(names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], id=None), length=-1, id=None),\n",
       " 'chunk_tags': Sequence(feature=ClassLabel(names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use ```gensim``` to get some pretrained word embeddings for the input layer to the model. \n",
    "\n",
    "In this example, we're going to use a GloVe model pretrained on Wikipedia, with 50 dimensions.\n",
    "\n",
    "I've provided a helper function to take the ```gensim``` embeddings and prepare them for ```pytorch```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# CONVERTING EMBEDDINGS\n",
    "model = api.load(\"glove-wiki-gigaword-50\") # loading an unsupervised learning algorithm for obtaining vector representations of words\n",
    "\n",
    "# convert gensim word embedding to torch word embedding\n",
    "embedding_layer, vocab = gensim_to_torch_embedding(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mgensim_to_torch_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgensim_wv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "- Add type hints on input and output\n",
      "- add function description\n",
      "- understand the pad and unk embeddings, add an argument which makes these optional. \n",
      "    E.g. add_padding = True and add_unknown = True\n",
      "\u001b[0;31mFile:\u001b[0m      /work/NLP-AU/src/embedding.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "?gensim_to_torch_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a batch\n",
    "\n",
    "The first thing we want to do is to shuffle our dataset before training. \n",
    "\n",
    "Why might it be a good idea to shuffle the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling training data, both before training and between epochs, helps prevent model overfitting by ensuring that batches are more representative of the entire dataset (in batch gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "shuffled_train = dataset[\"train\"].shuffle(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to bundle the shuffled training data into smaller batches of predefined size. I've written a small utility function here to help. \n",
    "\n",
    "<details>\n",
    "<summary>Q: Can you explain how the ```batch()``` function works?</summary>\n",
    "<br>\n",
    " Hint: Check out [this link](https://realpython.com/introduction-to-python-generators/).\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The batch function \n",
    "Creates batches from an iterable dataset and with a specified bach size. \n",
    "\n",
    "First, it returns an iterator of the data set specified. An iterator is an object that contains a countable number of values.\n",
    "Then it returns an iterable of tuples of size equal to batch_sizd\n",
    "The batch size refers to, not number of batches, but to the size of each batch. number of batches depends on batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batches_tokens = batch(shuffled_train[\"tokens\"], batch_size)\n",
    "batches_tags = batch(shuffled_train[\"ner_tags\"], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to use the ```tokens_to_idx()``` function below on our batches.\n",
    "\n",
    "<details>\n",
    "<summary>Q: What is this function doing? Why is it doing it?</summary>\n",
    "<br>\n",
    "We're making everything lowercase and adding a new, arbitrary token called <UNK> to the vocabulary. This <UNK> means \"unknown\" and is used to replace out-of-vocabulary tokens in the data - i.e. tokens that don't appear in the vocabulary of the pretrained word embeddings.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function takes the input #tokens and vocab. vocab calls, from model specified aboce, key_to_index which is a dictionary of words and their given values. \n",
    "\n",
    "the .get method returns all that is inputted to it: returns the value of the item with the specified key.\n",
    "the t.lower() returns strings where all cases are lower case.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_idx(tokens, vocab = model.key_to_index):\n",
    "    \"\"\"\n",
    "    - Write documentation for this function including type hints for each argument and return statement\n",
    "    - What does the .get method do?\n",
    "    - Why lowercase?\n",
    "    \"\"\"\n",
    "    return [vocab.get(t.lower(), vocab[\"UNK\"]) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'a': 7,\n",
       " '\"': 8,\n",
       " \"'s\": 9,\n",
       " 'for': 10,\n",
       " '-': 11,\n",
       " 'that': 12,\n",
       " 'on': 13,\n",
       " 'is': 14,\n",
       " 'was': 15,\n",
       " 'said': 16,\n",
       " 'with': 17,\n",
       " 'he': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'by': 21,\n",
       " 'at': 22,\n",
       " '(': 23,\n",
       " ')': 24,\n",
       " 'from': 25,\n",
       " 'his': 26,\n",
       " \"''\": 27,\n",
       " '``': 28,\n",
       " 'an': 29,\n",
       " 'be': 30,\n",
       " 'has': 31,\n",
       " 'are': 32,\n",
       " 'have': 33,\n",
       " 'but': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'this': 37,\n",
       " 'who': 38,\n",
       " 'they': 39,\n",
       " 'had': 40,\n",
       " 'i': 41,\n",
       " 'which': 42,\n",
       " 'will': 43,\n",
       " 'their': 44,\n",
       " ':': 45,\n",
       " 'or': 46,\n",
       " 'its': 47,\n",
       " 'one': 48,\n",
       " 'after': 49,\n",
       " 'new': 50,\n",
       " 'been': 51,\n",
       " 'also': 52,\n",
       " 'we': 53,\n",
       " 'would': 54,\n",
       " 'two': 55,\n",
       " 'more': 56,\n",
       " \"'\": 57,\n",
       " 'first': 58,\n",
       " 'about': 59,\n",
       " 'up': 60,\n",
       " 'when': 61,\n",
       " 'year': 62,\n",
       " 'there': 63,\n",
       " 'all': 64,\n",
       " '--': 65,\n",
       " 'out': 66,\n",
       " 'she': 67,\n",
       " 'other': 68,\n",
       " 'people': 69,\n",
       " \"n't\": 70,\n",
       " 'her': 71,\n",
       " 'percent': 72,\n",
       " 'than': 73,\n",
       " 'over': 74,\n",
       " 'into': 75,\n",
       " 'last': 76,\n",
       " 'some': 77,\n",
       " 'government': 78,\n",
       " 'time': 79,\n",
       " '$': 80,\n",
       " 'you': 81,\n",
       " 'years': 82,\n",
       " 'if': 83,\n",
       " 'no': 84,\n",
       " 'world': 85,\n",
       " 'can': 86,\n",
       " 'three': 87,\n",
       " 'do': 88,\n",
       " ';': 89,\n",
       " 'president': 90,\n",
       " 'only': 91,\n",
       " 'state': 92,\n",
       " 'million': 93,\n",
       " 'could': 94,\n",
       " 'us': 95,\n",
       " 'most': 96,\n",
       " '_': 97,\n",
       " 'against': 98,\n",
       " 'u.s.': 99,\n",
       " 'so': 100,\n",
       " 'them': 101,\n",
       " 'what': 102,\n",
       " 'him': 103,\n",
       " 'united': 104,\n",
       " 'during': 105,\n",
       " 'before': 106,\n",
       " 'may': 107,\n",
       " 'since': 108,\n",
       " 'many': 109,\n",
       " 'while': 110,\n",
       " 'where': 111,\n",
       " 'states': 112,\n",
       " 'because': 113,\n",
       " 'now': 114,\n",
       " 'city': 115,\n",
       " 'made': 116,\n",
       " 'like': 117,\n",
       " 'between': 118,\n",
       " 'did': 119,\n",
       " 'just': 120,\n",
       " 'national': 121,\n",
       " 'day': 122,\n",
       " 'country': 123,\n",
       " 'under': 124,\n",
       " 'such': 125,\n",
       " 'second': 126,\n",
       " 'then': 127,\n",
       " 'company': 128,\n",
       " 'group': 129,\n",
       " 'any': 130,\n",
       " 'through': 131,\n",
       " 'china': 132,\n",
       " 'four': 133,\n",
       " 'being': 134,\n",
       " 'down': 135,\n",
       " 'war': 136,\n",
       " 'back': 137,\n",
       " 'off': 138,\n",
       " 'south': 139,\n",
       " 'american': 140,\n",
       " 'minister': 141,\n",
       " 'police': 142,\n",
       " 'well': 143,\n",
       " 'including': 144,\n",
       " 'team': 145,\n",
       " 'international': 146,\n",
       " 'week': 147,\n",
       " 'officials': 148,\n",
       " 'still': 149,\n",
       " 'both': 150,\n",
       " 'even': 151,\n",
       " 'high': 152,\n",
       " 'part': 153,\n",
       " 'told': 154,\n",
       " 'those': 155,\n",
       " 'end': 156,\n",
       " 'former': 157,\n",
       " 'these': 158,\n",
       " 'make': 159,\n",
       " 'billion': 160,\n",
       " 'work': 161,\n",
       " 'our': 162,\n",
       " 'home': 163,\n",
       " 'school': 164,\n",
       " 'party': 165,\n",
       " 'house': 166,\n",
       " 'old': 167,\n",
       " 'later': 168,\n",
       " 'get': 169,\n",
       " 'another': 170,\n",
       " 'tuesday': 171,\n",
       " 'news': 172,\n",
       " 'long': 173,\n",
       " 'five': 174,\n",
       " 'called': 175,\n",
       " '1': 176,\n",
       " 'wednesday': 177,\n",
       " 'military': 178,\n",
       " 'way': 179,\n",
       " 'used': 180,\n",
       " 'much': 181,\n",
       " 'next': 182,\n",
       " 'monday': 183,\n",
       " 'thursday': 184,\n",
       " 'friday': 185,\n",
       " 'game': 186,\n",
       " 'here': 187,\n",
       " '?': 188,\n",
       " 'should': 189,\n",
       " 'take': 190,\n",
       " 'very': 191,\n",
       " 'my': 192,\n",
       " 'north': 193,\n",
       " 'security': 194,\n",
       " 'season': 195,\n",
       " 'york': 196,\n",
       " 'how': 197,\n",
       " 'public': 198,\n",
       " 'early': 199,\n",
       " 'according': 200,\n",
       " 'several': 201,\n",
       " 'court': 202,\n",
       " 'say': 203,\n",
       " 'around': 204,\n",
       " 'foreign': 205,\n",
       " '10': 206,\n",
       " 'until': 207,\n",
       " 'set': 208,\n",
       " 'political': 209,\n",
       " 'says': 210,\n",
       " 'market': 211,\n",
       " 'however': 212,\n",
       " 'family': 213,\n",
       " 'life': 214,\n",
       " 'same': 215,\n",
       " 'general': 216,\n",
       " '–': 217,\n",
       " 'left': 218,\n",
       " 'good': 219,\n",
       " 'top': 220,\n",
       " 'university': 221,\n",
       " 'going': 222,\n",
       " 'number': 223,\n",
       " 'major': 224,\n",
       " 'known': 225,\n",
       " 'points': 226,\n",
       " 'won': 227,\n",
       " 'six': 228,\n",
       " 'month': 229,\n",
       " 'dollars': 230,\n",
       " 'bank': 231,\n",
       " '2': 232,\n",
       " 'iraq': 233,\n",
       " 'use': 234,\n",
       " 'members': 235,\n",
       " 'each': 236,\n",
       " 'area': 237,\n",
       " 'found': 238,\n",
       " 'official': 239,\n",
       " 'sunday': 240,\n",
       " 'place': 241,\n",
       " 'go': 242,\n",
       " 'based': 243,\n",
       " 'among': 244,\n",
       " 'third': 245,\n",
       " 'times': 246,\n",
       " 'took': 247,\n",
       " 'right': 248,\n",
       " 'days': 249,\n",
       " 'local': 250,\n",
       " 'economic': 251,\n",
       " 'countries': 252,\n",
       " 'see': 253,\n",
       " 'best': 254,\n",
       " 'report': 255,\n",
       " 'killed': 256,\n",
       " 'held': 257,\n",
       " 'business': 258,\n",
       " 'west': 259,\n",
       " 'does': 260,\n",
       " 'own': 261,\n",
       " '%': 262,\n",
       " 'came': 263,\n",
       " 'law': 264,\n",
       " 'months': 265,\n",
       " 'women': 266,\n",
       " \"'re\": 267,\n",
       " 'power': 268,\n",
       " 'think': 269,\n",
       " 'service': 270,\n",
       " 'children': 271,\n",
       " 'bush': 272,\n",
       " 'show': 273,\n",
       " '/': 274,\n",
       " 'help': 275,\n",
       " 'chief': 276,\n",
       " 'saturday': 277,\n",
       " 'system': 278,\n",
       " 'john': 279,\n",
       " 'support': 280,\n",
       " 'series': 281,\n",
       " 'play': 282,\n",
       " 'office': 283,\n",
       " 'following': 284,\n",
       " 'me': 285,\n",
       " 'meeting': 286,\n",
       " 'expected': 287,\n",
       " 'late': 288,\n",
       " 'washington': 289,\n",
       " 'games': 290,\n",
       " 'european': 291,\n",
       " 'league': 292,\n",
       " 'reported': 293,\n",
       " 'final': 294,\n",
       " 'added': 295,\n",
       " 'without': 296,\n",
       " 'british': 297,\n",
       " 'white': 298,\n",
       " 'history': 299,\n",
       " 'man': 300,\n",
       " 'men': 301,\n",
       " 'became': 302,\n",
       " 'want': 303,\n",
       " 'march': 304,\n",
       " 'case': 305,\n",
       " 'few': 306,\n",
       " 'run': 307,\n",
       " 'money': 308,\n",
       " 'began': 309,\n",
       " 'open': 310,\n",
       " 'name': 311,\n",
       " 'trade': 312,\n",
       " 'center': 313,\n",
       " '3': 314,\n",
       " 'israel': 315,\n",
       " 'oil': 316,\n",
       " 'too': 317,\n",
       " 'al': 318,\n",
       " 'film': 319,\n",
       " 'win': 320,\n",
       " 'led': 321,\n",
       " 'east': 322,\n",
       " 'central': 323,\n",
       " '20': 324,\n",
       " 'air': 325,\n",
       " 'come': 326,\n",
       " 'chinese': 327,\n",
       " 'town': 328,\n",
       " 'leader': 329,\n",
       " 'army': 330,\n",
       " 'line': 331,\n",
       " 'never': 332,\n",
       " 'little': 333,\n",
       " 'played': 334,\n",
       " 'prime': 335,\n",
       " 'death': 336,\n",
       " 'companies': 337,\n",
       " 'least': 338,\n",
       " 'put': 339,\n",
       " 'forces': 340,\n",
       " 'past': 341,\n",
       " 'de': 342,\n",
       " 'half': 343,\n",
       " 'june': 344,\n",
       " 'saying': 345,\n",
       " 'know': 346,\n",
       " 'federal': 347,\n",
       " 'french': 348,\n",
       " 'peace': 349,\n",
       " 'earlier': 350,\n",
       " 'capital': 351,\n",
       " 'force': 352,\n",
       " 'great': 353,\n",
       " 'union': 354,\n",
       " 'near': 355,\n",
       " 'released': 356,\n",
       " 'small': 357,\n",
       " 'department': 358,\n",
       " 'every': 359,\n",
       " 'health': 360,\n",
       " 'japan': 361,\n",
       " 'head': 362,\n",
       " 'ago': 363,\n",
       " 'night': 364,\n",
       " 'big': 365,\n",
       " 'cup': 366,\n",
       " 'election': 367,\n",
       " 'region': 368,\n",
       " 'director': 369,\n",
       " 'talks': 370,\n",
       " 'program': 371,\n",
       " 'far': 372,\n",
       " 'today': 373,\n",
       " 'statement': 374,\n",
       " 'july': 375,\n",
       " 'although': 376,\n",
       " 'district': 377,\n",
       " 'again': 378,\n",
       " 'born': 379,\n",
       " 'development': 380,\n",
       " 'leaders': 381,\n",
       " 'council': 382,\n",
       " 'close': 383,\n",
       " 'record': 384,\n",
       " 'along': 385,\n",
       " 'county': 386,\n",
       " 'france': 387,\n",
       " 'went': 388,\n",
       " 'point': 389,\n",
       " 'must': 390,\n",
       " 'spokesman': 391,\n",
       " 'your': 392,\n",
       " 'member': 393,\n",
       " 'plan': 394,\n",
       " 'financial': 395,\n",
       " 'april': 396,\n",
       " 'recent': 397,\n",
       " 'campaign': 398,\n",
       " 'become': 399,\n",
       " 'troops': 400,\n",
       " 'whether': 401,\n",
       " 'lost': 402,\n",
       " 'music': 403,\n",
       " '15': 404,\n",
       " 'got': 405,\n",
       " 'israeli': 406,\n",
       " '30': 407,\n",
       " 'need': 408,\n",
       " '4': 409,\n",
       " 'lead': 410,\n",
       " 'already': 411,\n",
       " 'russia': 412,\n",
       " 'though': 413,\n",
       " 'might': 414,\n",
       " 'free': 415,\n",
       " 'hit': 416,\n",
       " 'rights': 417,\n",
       " '11': 418,\n",
       " 'information': 419,\n",
       " 'away': 420,\n",
       " '12': 421,\n",
       " '5': 422,\n",
       " 'others': 423,\n",
       " 'control': 424,\n",
       " 'within': 425,\n",
       " 'large': 426,\n",
       " 'economy': 427,\n",
       " 'press': 428,\n",
       " 'agency': 429,\n",
       " 'water': 430,\n",
       " 'died': 431,\n",
       " 'career': 432,\n",
       " 'making': 433,\n",
       " '...': 434,\n",
       " 'deal': 435,\n",
       " 'attack': 436,\n",
       " 'side': 437,\n",
       " 'seven': 438,\n",
       " 'better': 439,\n",
       " 'less': 440,\n",
       " 'september': 441,\n",
       " 'once': 442,\n",
       " 'clinton': 443,\n",
       " 'main': 444,\n",
       " 'due': 445,\n",
       " 'committee': 446,\n",
       " 'building': 447,\n",
       " 'conference': 448,\n",
       " 'club': 449,\n",
       " 'january': 450,\n",
       " 'decision': 451,\n",
       " 'stock': 452,\n",
       " 'america': 453,\n",
       " 'given': 454,\n",
       " 'give': 455,\n",
       " 'often': 456,\n",
       " 'announced': 457,\n",
       " 'television': 458,\n",
       " 'industry': 459,\n",
       " 'order': 460,\n",
       " 'young': 461,\n",
       " \"'ve\": 462,\n",
       " 'palestinian': 463,\n",
       " 'age': 464,\n",
       " 'start': 465,\n",
       " 'administration': 466,\n",
       " 'russian': 467,\n",
       " 'prices': 468,\n",
       " 'round': 469,\n",
       " 'december': 470,\n",
       " 'nations': 471,\n",
       " \"'m\": 472,\n",
       " 'human': 473,\n",
       " 'india': 474,\n",
       " 'defense': 475,\n",
       " 'asked': 476,\n",
       " 'total': 477,\n",
       " 'october': 478,\n",
       " 'players': 479,\n",
       " 'bill': 480,\n",
       " 'important': 481,\n",
       " 'southern': 482,\n",
       " 'move': 483,\n",
       " 'fire': 484,\n",
       " 'population': 485,\n",
       " 'rose': 486,\n",
       " 'november': 487,\n",
       " 'include': 488,\n",
       " 'further': 489,\n",
       " 'nuclear': 490,\n",
       " 'street': 491,\n",
       " 'taken': 492,\n",
       " 'media': 493,\n",
       " 'different': 494,\n",
       " 'issue': 495,\n",
       " 'received': 496,\n",
       " 'secretary': 497,\n",
       " 'return': 498,\n",
       " 'college': 499,\n",
       " 'working': 500,\n",
       " 'community': 501,\n",
       " 'eight': 502,\n",
       " 'groups': 503,\n",
       " 'despite': 504,\n",
       " 'level': 505,\n",
       " 'largest': 506,\n",
       " 'whose': 507,\n",
       " 'attacks': 508,\n",
       " 'germany': 509,\n",
       " 'august': 510,\n",
       " 'change': 511,\n",
       " 'church': 512,\n",
       " 'nation': 513,\n",
       " 'german': 514,\n",
       " 'station': 515,\n",
       " 'london': 516,\n",
       " 'weeks': 517,\n",
       " 'having': 518,\n",
       " '18': 519,\n",
       " 'research': 520,\n",
       " 'black': 521,\n",
       " 'services': 522,\n",
       " 'story': 523,\n",
       " '6': 524,\n",
       " 'europe': 525,\n",
       " 'sales': 526,\n",
       " 'policy': 527,\n",
       " 'visit': 528,\n",
       " 'northern': 529,\n",
       " 'lot': 530,\n",
       " 'across': 531,\n",
       " 'per': 532,\n",
       " 'current': 533,\n",
       " 'board': 534,\n",
       " 'football': 535,\n",
       " 'ministry': 536,\n",
       " 'workers': 537,\n",
       " 'vote': 538,\n",
       " 'book': 539,\n",
       " 'fell': 540,\n",
       " 'seen': 541,\n",
       " 'role': 542,\n",
       " 'students': 543,\n",
       " 'shares': 544,\n",
       " 'iran': 545,\n",
       " 'process': 546,\n",
       " 'agreement': 547,\n",
       " 'quarter': 548,\n",
       " 'full': 549,\n",
       " 'match': 550,\n",
       " 'started': 551,\n",
       " 'growth': 552,\n",
       " 'yet': 553,\n",
       " 'moved': 554,\n",
       " 'possible': 555,\n",
       " 'western': 556,\n",
       " 'special': 557,\n",
       " '100': 558,\n",
       " 'plans': 559,\n",
       " 'interest': 560,\n",
       " 'behind': 561,\n",
       " 'strong': 562,\n",
       " 'england': 563,\n",
       " 'named': 564,\n",
       " 'food': 565,\n",
       " 'period': 566,\n",
       " 'real': 567,\n",
       " 'authorities': 568,\n",
       " 'car': 569,\n",
       " 'term': 570,\n",
       " 'rate': 571,\n",
       " 'race': 572,\n",
       " 'nearly': 573,\n",
       " 'korea': 574,\n",
       " 'enough': 575,\n",
       " 'site': 576,\n",
       " 'opposition': 577,\n",
       " 'keep': 578,\n",
       " '25': 579,\n",
       " 'call': 580,\n",
       " 'future': 581,\n",
       " 'taking': 582,\n",
       " 'island': 583,\n",
       " '2008': 584,\n",
       " '2006': 585,\n",
       " 'road': 586,\n",
       " 'outside': 587,\n",
       " 'really': 588,\n",
       " 'century': 589,\n",
       " 'democratic': 590,\n",
       " 'almost': 591,\n",
       " 'single': 592,\n",
       " 'share': 593,\n",
       " 'leading': 594,\n",
       " 'trying': 595,\n",
       " 'find': 596,\n",
       " 'album': 597,\n",
       " 'senior': 598,\n",
       " 'minutes': 599,\n",
       " 'together': 600,\n",
       " 'congress': 601,\n",
       " 'index': 602,\n",
       " 'australia': 603,\n",
       " 'results': 604,\n",
       " 'hard': 605,\n",
       " 'hours': 606,\n",
       " 'land': 607,\n",
       " 'action': 608,\n",
       " 'higher': 609,\n",
       " 'field': 610,\n",
       " 'cut': 611,\n",
       " 'coach': 612,\n",
       " 'elections': 613,\n",
       " 'san': 614,\n",
       " 'issues': 615,\n",
       " 'executive': 616,\n",
       " 'february': 617,\n",
       " 'production': 618,\n",
       " 'areas': 619,\n",
       " 'river': 620,\n",
       " 'face': 621,\n",
       " 'using': 622,\n",
       " 'japanese': 623,\n",
       " 'province': 624,\n",
       " 'park': 625,\n",
       " 'price': 626,\n",
       " 'commission': 627,\n",
       " 'california': 628,\n",
       " 'father': 629,\n",
       " 'son': 630,\n",
       " 'education': 631,\n",
       " '7': 632,\n",
       " 'village': 633,\n",
       " 'energy': 634,\n",
       " 'shot': 635,\n",
       " 'short': 636,\n",
       " 'africa': 637,\n",
       " 'key': 638,\n",
       " 'red': 639,\n",
       " 'association': 640,\n",
       " 'average': 641,\n",
       " 'pay': 642,\n",
       " 'exchange': 643,\n",
       " 'eu': 644,\n",
       " 'something': 645,\n",
       " 'gave': 646,\n",
       " 'likely': 647,\n",
       " 'player': 648,\n",
       " 'george': 649,\n",
       " '2007': 650,\n",
       " 'victory': 651,\n",
       " '8': 652,\n",
       " 'low': 653,\n",
       " 'things': 654,\n",
       " '2010': 655,\n",
       " 'pakistan': 656,\n",
       " '14': 657,\n",
       " 'post': 658,\n",
       " 'social': 659,\n",
       " 'continue': 660,\n",
       " 'ever': 661,\n",
       " 'look': 662,\n",
       " 'chairman': 663,\n",
       " 'job': 664,\n",
       " '2000': 665,\n",
       " 'soldiers': 666,\n",
       " 'able': 667,\n",
       " 'parliament': 668,\n",
       " 'front': 669,\n",
       " 'himself': 670,\n",
       " 'problems': 671,\n",
       " 'private': 672,\n",
       " 'lower': 673,\n",
       " 'list': 674,\n",
       " 'built': 675,\n",
       " '13': 676,\n",
       " 'efforts': 677,\n",
       " 'dollar': 678,\n",
       " 'miles': 679,\n",
       " 'included': 680,\n",
       " 'radio': 681,\n",
       " 'live': 682,\n",
       " 'form': 683,\n",
       " 'david': 684,\n",
       " 'african': 685,\n",
       " 'increase': 686,\n",
       " 'reports': 687,\n",
       " 'sent': 688,\n",
       " 'fourth': 689,\n",
       " 'always': 690,\n",
       " 'king': 691,\n",
       " '50': 692,\n",
       " 'tax': 693,\n",
       " 'taiwan': 694,\n",
       " 'britain': 695,\n",
       " '16': 696,\n",
       " 'playing': 697,\n",
       " 'title': 698,\n",
       " 'middle': 699,\n",
       " 'meet': 700,\n",
       " 'global': 701,\n",
       " 'wife': 702,\n",
       " '2009': 703,\n",
       " 'position': 704,\n",
       " 'located': 705,\n",
       " 'clear': 706,\n",
       " 'ahead': 707,\n",
       " '2004': 708,\n",
       " '2005': 709,\n",
       " 'iraqi': 710,\n",
       " 'english': 711,\n",
       " 'result': 712,\n",
       " 'release': 713,\n",
       " 'violence': 714,\n",
       " 'goal': 715,\n",
       " 'project': 716,\n",
       " 'closed': 717,\n",
       " 'border': 718,\n",
       " 'body': 719,\n",
       " 'soon': 720,\n",
       " 'crisis': 721,\n",
       " 'division': 722,\n",
       " '&amp;': 723,\n",
       " 'served': 724,\n",
       " 'tour': 725,\n",
       " 'hospital': 726,\n",
       " 'kong': 727,\n",
       " 'test': 728,\n",
       " 'hong': 729,\n",
       " 'u.n.': 730,\n",
       " 'inc.': 731,\n",
       " 'technology': 732,\n",
       " 'believe': 733,\n",
       " 'organization': 734,\n",
       " 'published': 735,\n",
       " 'weapons': 736,\n",
       " 'agreed': 737,\n",
       " 'why': 738,\n",
       " 'nine': 739,\n",
       " 'summer': 740,\n",
       " 'wanted': 741,\n",
       " 'republican': 742,\n",
       " 'act': 743,\n",
       " 'recently': 744,\n",
       " 'texas': 745,\n",
       " 'course': 746,\n",
       " 'problem': 747,\n",
       " 'senate': 748,\n",
       " 'medical': 749,\n",
       " 'un': 750,\n",
       " 'done': 751,\n",
       " 'reached': 752,\n",
       " 'star': 753,\n",
       " 'continued': 754,\n",
       " 'investors': 755,\n",
       " 'living': 756,\n",
       " 'care': 757,\n",
       " 'signed': 758,\n",
       " '17': 759,\n",
       " 'art': 760,\n",
       " 'provide': 761,\n",
       " 'worked': 762,\n",
       " 'presidential': 763,\n",
       " 'gold': 764,\n",
       " 'obama': 765,\n",
       " 'morning': 766,\n",
       " 'dead': 767,\n",
       " 'opened': 768,\n",
       " \"'ll\": 769,\n",
       " 'event': 770,\n",
       " 'previous': 771,\n",
       " 'cost': 772,\n",
       " 'instead': 773,\n",
       " 'canada': 774,\n",
       " 'band': 775,\n",
       " 'teams': 776,\n",
       " 'daily': 777,\n",
       " '2001': 778,\n",
       " 'available': 779,\n",
       " 'drug': 780,\n",
       " 'coming': 781,\n",
       " '2003': 782,\n",
       " 'investment': 783,\n",
       " '’s': 784,\n",
       " 'michael': 785,\n",
       " 'civil': 786,\n",
       " 'woman': 787,\n",
       " 'training': 788,\n",
       " 'appeared': 789,\n",
       " '9': 790,\n",
       " 'involved': 791,\n",
       " 'indian': 792,\n",
       " 'similar': 793,\n",
       " 'situation': 794,\n",
       " '24': 795,\n",
       " 'los': 796,\n",
       " 'running': 797,\n",
       " 'fighting': 798,\n",
       " 'mark': 799,\n",
       " '40': 800,\n",
       " 'trial': 801,\n",
       " 'hold': 802,\n",
       " 'australian': 803,\n",
       " 'thought': 804,\n",
       " '!': 805,\n",
       " 'study': 806,\n",
       " 'fall': 807,\n",
       " 'mother': 808,\n",
       " 'met': 809,\n",
       " 'relations': 810,\n",
       " 'anti': 811,\n",
       " '2002': 812,\n",
       " 'song': 813,\n",
       " 'popular': 814,\n",
       " 'base': 815,\n",
       " 'tv': 816,\n",
       " 'ground': 817,\n",
       " 'markets': 818,\n",
       " 'ii': 819,\n",
       " 'newspaper': 820,\n",
       " 'staff': 821,\n",
       " 'saw': 822,\n",
       " 'hand': 823,\n",
       " 'hope': 824,\n",
       " 'operations': 825,\n",
       " 'pressure': 826,\n",
       " 'americans': 827,\n",
       " 'eastern': 828,\n",
       " 'st.': 829,\n",
       " 'legal': 830,\n",
       " 'asia': 831,\n",
       " 'budget': 832,\n",
       " 'returned': 833,\n",
       " 'considered': 834,\n",
       " 'love': 835,\n",
       " 'wrote': 836,\n",
       " 'stop': 837,\n",
       " 'fight': 838,\n",
       " 'currently': 839,\n",
       " 'charges': 840,\n",
       " 'try': 841,\n",
       " 'aid': 842,\n",
       " 'ended': 843,\n",
       " 'management': 844,\n",
       " 'brought': 845,\n",
       " 'cases': 846,\n",
       " 'decided': 847,\n",
       " 'failed': 848,\n",
       " 'network': 849,\n",
       " 'works': 850,\n",
       " 'gas': 851,\n",
       " 'turned': 852,\n",
       " 'fact': 853,\n",
       " 'vice': 854,\n",
       " 'ca': 855,\n",
       " 'mexico': 856,\n",
       " 'trading': 857,\n",
       " 'especially': 858,\n",
       " 'reporters': 859,\n",
       " 'afghanistan': 860,\n",
       " 'common': 861,\n",
       " 'looking': 862,\n",
       " 'space': 863,\n",
       " 'rates': 864,\n",
       " 'manager': 865,\n",
       " 'loss': 866,\n",
       " '2011': 867,\n",
       " 'justice': 868,\n",
       " 'thousands': 869,\n",
       " 'james': 870,\n",
       " 'rather': 871,\n",
       " 'fund': 872,\n",
       " 'thing': 873,\n",
       " 'republic': 874,\n",
       " 'opening': 875,\n",
       " 'accused': 876,\n",
       " 'winning': 877,\n",
       " 'scored': 878,\n",
       " 'championship': 879,\n",
       " 'example': 880,\n",
       " 'getting': 881,\n",
       " 'biggest': 882,\n",
       " 'performance': 883,\n",
       " 'sports': 884,\n",
       " '1998': 885,\n",
       " 'let': 886,\n",
       " 'allowed': 887,\n",
       " 'schools': 888,\n",
       " 'means': 889,\n",
       " 'turn': 890,\n",
       " 'leave': 891,\n",
       " 'no.': 892,\n",
       " 'robert': 893,\n",
       " 'personal': 894,\n",
       " 'stocks': 895,\n",
       " 'showed': 896,\n",
       " 'light': 897,\n",
       " 'arrested': 898,\n",
       " 'person': 899,\n",
       " 'either': 900,\n",
       " 'offer': 901,\n",
       " 'majority': 902,\n",
       " 'battle': 903,\n",
       " '19': 904,\n",
       " 'class': 905,\n",
       " 'evidence': 906,\n",
       " 'makes': 907,\n",
       " 'society': 908,\n",
       " 'products': 909,\n",
       " 'regional': 910,\n",
       " 'needed': 911,\n",
       " 'stage': 912,\n",
       " 'am': 913,\n",
       " 'doing': 914,\n",
       " 'families': 915,\n",
       " 'construction': 916,\n",
       " 'various': 917,\n",
       " '1996': 918,\n",
       " 'sold': 919,\n",
       " 'independent': 920,\n",
       " 'kind': 921,\n",
       " 'airport': 922,\n",
       " 'paul': 923,\n",
       " 'judge': 924,\n",
       " 'internet': 925,\n",
       " 'movement': 926,\n",
       " 'room': 927,\n",
       " 'followed': 928,\n",
       " 'original': 929,\n",
       " 'angeles': 930,\n",
       " 'italy': 931,\n",
       " '`': 932,\n",
       " 'data': 933,\n",
       " 'comes': 934,\n",
       " 'parties': 935,\n",
       " 'nothing': 936,\n",
       " 'sea': 937,\n",
       " 'bring': 938,\n",
       " '2012': 939,\n",
       " 'annual': 940,\n",
       " 'officer': 941,\n",
       " 'beijing': 942,\n",
       " 'present': 943,\n",
       " 'remain': 944,\n",
       " 'nato': 945,\n",
       " '1999': 946,\n",
       " '22': 947,\n",
       " 'remains': 948,\n",
       " 'allow': 949,\n",
       " 'florida': 950,\n",
       " 'computer': 951,\n",
       " '21': 952,\n",
       " 'contract': 953,\n",
       " 'coast': 954,\n",
       " 'created': 955,\n",
       " 'demand': 956,\n",
       " 'operation': 957,\n",
       " 'events': 958,\n",
       " 'islamic': 959,\n",
       " 'beat': 960,\n",
       " 'analysts': 961,\n",
       " 'interview': 962,\n",
       " 'helped': 963,\n",
       " 'child': 964,\n",
       " 'probably': 965,\n",
       " 'spent': 966,\n",
       " 'asian': 967,\n",
       " 'effort': 968,\n",
       " 'cooperation': 969,\n",
       " 'shows': 970,\n",
       " 'calls': 971,\n",
       " 'investigation': 972,\n",
       " 'lives': 973,\n",
       " 'video': 974,\n",
       " 'yen': 975,\n",
       " 'runs': 976,\n",
       " 'tried': 977,\n",
       " 'bad': 978,\n",
       " 'described': 979,\n",
       " '1994': 980,\n",
       " 'toward': 981,\n",
       " 'written': 982,\n",
       " 'throughout': 983,\n",
       " 'established': 984,\n",
       " 'mission': 985,\n",
       " 'associated': 986,\n",
       " 'buy': 987,\n",
       " 'growing': 988,\n",
       " 'green': 989,\n",
       " 'forward': 990,\n",
       " 'competition': 991,\n",
       " 'poor': 992,\n",
       " 'latest': 993,\n",
       " 'banks': 994,\n",
       " 'question': 995,\n",
       " '1997': 996,\n",
       " 'prison': 997,\n",
       " 'feel': 998,\n",
       " 'attention': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.key_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll check below that everything is working as expected as expected by testing it on a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using only the first batch\n",
    "batch_tokens = next(batches_tokens)\n",
    "batch_tags = next(batches_tags)\n",
    "batch_tok_idx = [tokens_to_idx(sent) for sent in batch_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['shares', 'outstanding'],\n",
       " ['Oldham', '22', '9', '1', '12', '473', '681', '19'],\n",
       " ['Goldman',\n",
       "  'Sachs',\n",
       "  '&',\n",
       "  'Co',\n",
       "  'Wertpapier',\n",
       "  'GmbH',\n",
       "  'has',\n",
       "  'issued',\n",
       "  'a',\n",
       "  'total',\n",
       "  'of',\n",
       "  'five',\n",
       "  'million',\n",
       "  'American-style',\n",
       "  'call',\n",
       "  'warrants',\n",
       "  ',',\n",
       "  'on',\n",
       "  'Continental',\n",
       "  'AG',\n",
       "  ',',\n",
       "  'lead',\n",
       "  'manager',\n",
       "  'Goldman',\n",
       "  'Sachs',\n",
       "  '&',\n",
       "  'Co',\n",
       "  'said',\n",
       "  '.'],\n",
       " ['CINCINNATI', '64', '63', '.504', '3', '1/2'],\n",
       " ['MELBOURNE', '1996-08-23'],\n",
       " ['--',\n",
       "  'FLNC',\n",
       "  'Corsican',\n",
       "  'nationalist',\n",
       "  'movement',\n",
       "  'announces',\n",
       "  'end',\n",
       "  'of',\n",
       "  'truce',\n",
       "  'after',\n",
       "  'last',\n",
       "  'night',\n",
       "  \"'s\",\n",
       "  'attacks',\n",
       "  '.'],\n",
       " ['4-6', '7-6', '(', '7-4', ')'],\n",
       " ['Dealers',\n",
       "  'said',\n",
       "  'that',\n",
       "  'the',\n",
       "  'volume',\n",
       "  'of',\n",
       "  'longer-term',\n",
       "  'government',\n",
       "  'paper',\n",
       "  'declined',\n",
       "  'due',\n",
       "  'to',\n",
       "  'market',\n",
       "  'nervousness',\n",
       "  '.'],\n",
       " ['MONTREAL', 'AT', 'SAN', 'FRANCISCO'],\n",
       " ['\"',\n",
       "  'I',\n",
       "  'would',\n",
       "  'love',\n",
       "  'to',\n",
       "  'speak',\n",
       "  'about',\n",
       "  'everything',\n",
       "  ',',\n",
       "  '\"',\n",
       "  'said',\n",
       "  'Simpson',\n",
       "  ',',\n",
       "  'who',\n",
       "  'vowed',\n",
       "  'after',\n",
       "  'his',\n",
       "  'acquittal',\n",
       "  'to',\n",
       "  'find',\n",
       "  'the',\n",
       "  'killers',\n",
       "  'and',\n",
       "  'offered',\n",
       "  'a',\n",
       "  'substantial',\n",
       "  'reward',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'back-to-back',\n",
       "  'wins',\n",
       "  'vault',\n",
       "  'Ferrigato',\n",
       "  'from',\n",
       "  'sixth',\n",
       "  'to',\n",
       "  'second',\n",
       "  'in',\n",
       "  'the',\n",
       "  'overall',\n",
       "  'World',\n",
       "  'Cup',\n",
       "  'rankings',\n",
       "  'with',\n",
       "  '112',\n",
       "  'points',\n",
       "  'but',\n",
       "  'Museeuw',\n",
       "  'continues',\n",
       "  'to',\n",
       "  'hold',\n",
       "  'a',\n",
       "  'commanding',\n",
       "  'lead',\n",
       "  'with',\n",
       "  '162',\n",
       "  'points',\n",
       "  'after',\n",
       "  'eight',\n",
       "  'of',\n",
       "  'the',\n",
       "  '11',\n",
       "  'rounds',\n",
       "  '.'],\n",
       " ['Voters',\n",
       "  'will',\n",
       "  'be',\n",
       "  'choosing',\n",
       "  'a',\n",
       "  'three-member',\n",
       "  'presidency',\n",
       "  'and',\n",
       "  'a',\n",
       "  'parliament',\n",
       "  'to',\n",
       "  'rule',\n",
       "  'over',\n",
       "  'a',\n",
       "  'loose',\n",
       "  'union',\n",
       "  'of',\n",
       "  'Bosnia',\n",
       "  ',',\n",
       "  'comprised',\n",
       "  'of',\n",
       "  'a',\n",
       "  'Serb',\n",
       "  'republic',\n",
       "  'and',\n",
       "  'a',\n",
       "  'Moslem-Croat',\n",
       "  'federation',\n",
       "  '.'],\n",
       " ['*'],\n",
       " ['winning', 'percentage', ',', 'games', 'behind', 'first', 'place', ')'],\n",
       " ['This',\n",
       "  'compared',\n",
       "  'with',\n",
       "  'midweek',\n",
       "  'levels',\n",
       "  ',',\n",
       "  'before',\n",
       "  'the',\n",
       "  'welter',\n",
       "  'of',\n",
       "  'interest',\n",
       "  'rate',\n",
       "  'cuts',\n",
       "  ',',\n",
       "  'of',\n",
       "  '18.50',\n",
       "  'for',\n",
       "  'September',\n",
       "  ',',\n",
       "  '20.00',\n",
       "  'for',\n",
       "  'December',\n",
       "  ',',\n",
       "  '22.00',\n",
       "  'for',\n",
       "  'March',\n",
       "  'and',\n",
       "  '23.5',\n",
       "  'for',\n",
       "  'June',\n",
       "  ',',\n",
       "  'he',\n",
       "  'said',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'letter',\n",
       "  'made',\n",
       "  'reference',\n",
       "  'to',\n",
       "  'massacres',\n",
       "  'of',\n",
       "  'landless',\n",
       "  'peasants',\n",
       "  'in',\n",
       "  'August',\n",
       "  '1995',\n",
       "  'and',\n",
       "  'April',\n",
       "  '1996',\n",
       "  ',',\n",
       "  'which',\n",
       "  'claimed',\n",
       "  'the',\n",
       "  'lives',\n",
       "  'of',\n",
       "  '27',\n",
       "  'landless',\n",
       "  'peasants',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'suit',\n",
       "  'was',\n",
       "  'filed',\n",
       "  'on',\n",
       "  'Wednesday',\n",
       "  'in',\n",
       "  'federal',\n",
       "  'court',\n",
       "  'in',\n",
       "  'Manhattan',\n",
       "  'and',\n",
       "  'is',\n",
       "  'another',\n",
       "  'legal',\n",
       "  'skirmish',\n",
       "  'over',\n",
       "  'what',\n",
       "  'constitutes',\n",
       "  'a',\n",
       "  '\"',\n",
       "  'broadcast',\n",
       "  '\"',\n",
       "  'in',\n",
       "  'the',\n",
       "  'computer',\n",
       "  'age',\n",
       "  '.'],\n",
       " ['Marc',\n",
       "  'Dutroux',\n",
       "  ',',\n",
       "  'the',\n",
       "  'chief',\n",
       "  'accused',\n",
       "  'in',\n",
       "  'a',\n",
       "  'Belgian',\n",
       "  'child',\n",
       "  'murder',\n",
       "  'and',\n",
       "  'sex',\n",
       "  'abuse',\n",
       "  'scandal',\n",
       "  ',',\n",
       "  'is',\n",
       "  'suspected',\n",
       "  'of',\n",
       "  'murdering',\n",
       "  'a',\n",
       "  'young',\n",
       "  'Slovak',\n",
       "  'woman',\n",
       "  ',',\n",
       "  'the',\n",
       "  'Slovak',\n",
       "  'office',\n",
       "  'of',\n",
       "  'Interpol',\n",
       "  'said',\n",
       "  'on',\n",
       "  'Wednesday',\n",
       "  '.'],\n",
       " ['Year', 'to', 'June', '30', '.'],\n",
       " ['9.', 'Michael', 'Andersson', '(', 'Sweden', ')', 'Telekom', '54'],\n",
       " ['drawn',\n",
       "  ',',\n",
       "  'lost',\n",
       "  ',',\n",
       "  'goals',\n",
       "  'for',\n",
       "  ',',\n",
       "  'against',\n",
       "  ',',\n",
       "  'points',\n",
       "  ')',\n",
       "  ':'],\n",
       " ['Scorers',\n",
       "  ':',\n",
       "  'Petr',\n",
       "  'Gunda',\n",
       "  '(',\n",
       "  '1st',\n",
       "  'and',\n",
       "  '26th',\n",
       "  ')',\n",
       "  ',',\n",
       "  'Lumir',\n",
       "  'Mistr',\n",
       "  '(',\n",
       "  '19th',\n",
       "  ')',\n",
       "  ','],\n",
       " ['ALMERE', ',', 'Netherlands', '1996-08-28'],\n",
       " ['The',\n",
       "  'shot',\n",
       "  'brought',\n",
       "  'home',\n",
       "  'Ivan',\n",
       "  'Rodriguez',\n",
       "  ',',\n",
       "  'who',\n",
       "  'had',\n",
       "  'his',\n",
       "  'second',\n",
       "  'double',\n",
       "  'of',\n",
       "  'the',\n",
       "  'game',\n",
       "  ',',\n",
       "  'giving',\n",
       "  'him',\n",
       "  '42',\n",
       "  'this',\n",
       "  'season',\n",
       "  ',',\n",
       "  '41',\n",
       "  'as',\n",
       "  'a',\n",
       "  'catcher',\n",
       "  '.'],\n",
       " ['Fall',\n",
       "  'of',\n",
       "  'wickets',\n",
       "  ':',\n",
       "  '1-57',\n",
       "  '2-98',\n",
       "  '3-146',\n",
       "  '4-200',\n",
       "  '5-220',\n",
       "  '.'],\n",
       " ['\"',\n",
       "  'Both',\n",
       "  'countries',\n",
       "  \"'\",\n",
       "  'governments',\n",
       "  'were',\n",
       "  'formed',\n",
       "  'recently',\n",
       "  '.'],\n",
       " ['\"',\n",
       "  'When',\n",
       "  'the',\n",
       "  '(',\n",
       "  'profit',\n",
       "  ')',\n",
       "  'figures',\n",
       "  'will',\n",
       "  'bounce',\n",
       "  'back',\n",
       "  'up',\n",
       "  'again',\n",
       "  'is',\n",
       "  'just',\n",
       "  'a',\n",
       "  'function',\n",
       "  'of',\n",
       "  'markets',\n",
       "  'recovering',\n",
       "  'just',\n",
       "  'a',\n",
       "  'fraction',\n",
       "  ',',\n",
       "  '\"',\n",
       "  'he',\n",
       "  'added',\n",
       "  '.'],\n",
       " ['CHICAGO', '1996-08-27'],\n",
       " ['7.', 'Michel', 'Zanoli', '(', 'Netherlands', ')', 'MX', 'Onda'],\n",
       " ['15,000', '.'],\n",
       " ['CRICKET', '-', 'INDIA', 'BANS', 'SIDHU', 'FOR', '50', 'DAYS', '.'],\n",
       " ['2.', 'Ashia', 'Hansen', '(', 'Britain', ')', '14.78'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_tok_idx\n",
    "batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "next(iterator[, default])\n",
      "\n",
      "Return the next item from the iterator. If default is given and the iterator\n",
      "is exhausted, it is returned instead of raising StopIteration.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
     ]
    }
   ],
   "source": [
    "# the next() function Return the next item from the iterator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with document classification, our model needs to take input sequences of a fixed length. To get around this we do a couple of different steps.\n",
    "\n",
    "- Find the length of the longest sequence in the batch\n",
    "- Pad shorter sequences to the max length using an arbitrary token like <PAD>\n",
    "- Give the <PAD> token a new label ```-1``` to differentiate it from the other labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute length of longest sentence in batch\n",
    "batch_max_len = max([len(s) for s in batch_tok_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Can you figure out the logic of what is happening in the next two cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len))\n",
    "batch_labels = -1 * np.ones((batch_size, batch_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data to the numpy array\n",
    "for i in range(batch_size):\n",
    "    tok_idx = batch_tok_idx[i]\n",
    "    tags = batch_tags[i]\n",
    "    size = len(tok_idx)\n",
    "\n",
    "    batch_input[i][:size] = tok_idx\n",
    "    batch_labels[i][:size] = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to conver the arrays into ```pytorch``` tensors, ready for the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all data are indices, we convert them to torch LongTensors (integers)\n",
    "batch_input, batch_labels = torch.LongTensor(batch_input), torch.LongTensor(\n",
    "    batch_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data now batched and processed, we want to run it through our RNN the same way as when we trained a clasifier. Note that this cell is incomplete and won't yet run; that's part of the assignment!\n",
    "\n",
    "Q: Why is ```output_dim = num_classes + 1```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0membedding_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhidden_dim_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Base class for all neural network modules.\n",
      "\n",
      "Your models should also subclass this class.\n",
      "\n",
      "Modules can also contain other Modules, allowing to nest them in\n",
      "a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "    import torch.nn as nn\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    class Model(nn.Module):\n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "        def forward(self, x):\n",
      "            x = F.relu(self.conv1(x))\n",
      "            return F.relu(self.conv2(x))\n",
      "\n",
      "Submodules assigned in this way will be registered, and will have their\n",
      "parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      ".. note::\n",
      "    As per the example above, an ``__init__()`` call to the parent class\n",
      "    must be made before assignment on the child.\n",
      "\n",
      ":ivar training: Boolean represents whether this module is in training or\n",
      "                evaluation mode.\n",
      ":vartype training: bool\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           /work/NLP-AU/src/LSTM.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "?RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO:] Training classifier...\n",
      "epoch: 10, loss = 0.8666\n",
      "epoch: 20, loss = 0.7439\n",
      "epoch: 30, loss = 0.6424\n",
      "epoch: 40, loss = 0.6036\n",
      "epoch: 50, loss = 0.5650\n",
      "epoch: 60, loss = 0.5144\n",
      "epoch: 70, loss = 0.4540\n",
      "epoch: 80, loss = 0.3981\n",
      "epoch: 90, loss = 0.3403\n",
      "epoch: 100, loss = 0.2819\n",
      "[INFO:] Finished traning!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f637c3e89d0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CREATE MODEL\n",
    "model = RNN(\n",
    "    embedding_layer=embedding_layer, output_dim=num_classes + 1, hidden_dim_size=256\n",
    ")\n",
    "\n",
    "\n",
    "#loss_fn takes a prediction tensor and a target tensor as arguments\n",
    "# it returns the average loss over all observations in the batch\n",
    "# the outputs here are our prediction tensors\n",
    "\n",
    "\n",
    "\n",
    "# define loss and optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "\n",
    "# train\n",
    "epochs = 100\n",
    "print(\"[INFO:] Training classifier...\")\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    # forward\n",
    "    X = batch_input\n",
    "    y = model(X)\n",
    "\n",
    "    # backward\n",
    "    loss = model.loss_fn(outputs=y, labels=batch_labels)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    # take step, reset\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # some print to see that it is running\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"epoch: {epoch+1}, loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"[INFO:] Finished traning!\")\n",
    "\n",
    "loss_H = [val.item() for val in loss_history]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_H)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_dim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0membedding_layer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mhidden_dim_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Base class for all neural network modules.\n",
      "\n",
      "Your models should also subclass this class.\n",
      "\n",
      "Modules can also contain other Modules, allowing to nest them in\n",
      "a tree structure. You can assign the submodules as regular attributes::\n",
      "\n",
      "    import torch.nn as nn\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    class Model(nn.Module):\n",
      "        def __init__(self):\n",
      "            super().__init__()\n",
      "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
      "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
      "\n",
      "        def forward(self, x):\n",
      "            x = F.relu(self.conv1(x))\n",
      "            return F.relu(self.conv2(x))\n",
      "\n",
      "Submodules assigned in this way will be registered, and will have their\n",
      "parameters converted too when you call :meth:`to`, etc.\n",
      "\n",
      ".. note::\n",
      "    As per the example above, an ``__init__()`` call to the parent class\n",
      "    must be made before assignment on the child.\n",
      "\n",
      ":ivar training: Boolean represents whether this module is in training or\n",
      "                evaluation mode.\n",
      ":vartype training: bool\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m           /work/NLP-AU/src/LSTM.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "source": [
    "?RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating an LSTM with ```pytorch```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the file [LSTM.py](../src/LSTM.py), I've aready created an LSTM for you using ```pytorch```. Take some time to read through the code and make sure you understand how it's built up.\n",
    "\n",
    "Some questions for you to discuss in groups:\n",
    "\n",
    "- How is an LSTM layer created using ```pytorch```? How does the code compare to the classifier code you wrote last week?\n",
    "- What's going on with that weird bit that says ```@staticmethod```?\n",
    "  - [This might help](https://realpython.com/instance-class-and-static-methods-demystified/).\n",
    "- On the forward pass, we use ```log_softmax()``` to make output predictions. What is this, and how does it relate to the output from the sigmoid function that we used in the document classification?\n",
    "- How would we make this LSTM model *bidirectional* - i.e. make it a Bi-LSTM? \n",
    "  - Hint: Check the documentation for the LSTM layer on the ```pytorch``` website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
